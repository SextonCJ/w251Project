{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Labels\n",
    "\n",
    "Code to build dataframes that contain directory listings of train, test and validate extracted JPEG frames including labels  \n",
    "Code to build dataframes that contain directory listings of train, test and validate extracted avi videos including labels  \n",
    "Assumed working from DAiSEE dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/chris/MIDS/w251/w251_ChrisSexton/Project/src/cnn'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps = '1FPS'\n",
    "frame_dir = '../../data/DAiSEE/' + fps + '/DataSet/'\n",
    "label_path = '../../data/DAiSEE/Labels/'\n",
    "out_dir = '../../data/DAiSEE/' + fps + '/data' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Labels\n",
    "df_train_labels = pd.read_csv(label_path + 'TrainLabels.csv')\n",
    "df_train_labels['root'] = df_train_labels['ClipID'].str[:-4]\n",
    "\n",
    "df_test_labels = pd.read_csv(label_path + 'TestLabels.csv')\n",
    "df_test_labels['root'] = df_test_labels['ClipID'].str[:-4]\n",
    "\n",
    "df_val_labels = pd.read_csv(label_path + 'ValidationLabels.csv')\n",
    "df_val_labels['root'] = df_val_labels['ClipID'].str[:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5358, 6)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do some EDA on the labels. This is a multi label dataset - can it be simplified?\n",
    "df_train_labels.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Data Files\n",
    "# Train\n",
    "df_train_jpg = pd.DataFrame([file_path for file_path in Path(frame_dir + 'Train').glob('**/*.jpg')], columns=['file'])\n",
    "df_train_jpg[\"root\"] = df_train_jpg[\"file\"].apply(lambda x: os.path.split(os.path.split(x)[0])[1])\n",
    "# Test\n",
    "df_test_jpg = pd.DataFrame([file_path for file_path in Path(frame_dir + 'Test').glob('**/*.jpg')], columns=['file'])\n",
    "df_test_jpg[\"root\"] = df_test_jpg[\"file\"].apply(lambda x: os.path.split(os.path.split(x)[0])[1])\n",
    "# Validation\n",
    "df_val_jpg = pd.DataFrame([file_path for file_path in Path(frame_dir +'Validation').glob('**/*.jpg')], columns=['file'])\n",
    "df_val_jpg[\"root\"] = df_val_jpg[\"file\"].apply(lambda x: os.path.split(os.path.split(x)[0])[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109732, 2)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_jpg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge\n",
    "df_train = pd.merge(df_train_jpg, df_train_labels, on='root', how='left')\n",
    "df_test = pd.merge(df_test_jpg, df_test_labels, on='root', how='left')\n",
    "df_val = pd.merge(df_val_jpg, df_val_labels, on='root', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Base File Name\n",
    "df_train['basefile'] = df_train['file'].apply(lambda x: os.path.basename(x))\n",
    "df_test['basefile'] = df_test['file'].apply(lambda x: os.path.basename(x))\n",
    "df_val['basefile'] = df_val['file'].apply(lambda x: os.path.basename(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix Error in file name (exra space)\n",
    "df_train.rename(columns={'Frustration ':'Frustration'}, inplace = True)\n",
    "df_test.rename(columns={'Frustration ':'Frustration'}, inplace = True)\n",
    "df_val.rename(columns={'Frustration ':'Frustration'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(109732, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>root</th>\n",
       "      <th>ClipID</th>\n",
       "      <th>Boredom</th>\n",
       "      <th>Engagement</th>\n",
       "      <th>Confusion</th>\n",
       "      <th>Frustration</th>\n",
       "      <th>basefile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../../data/DAiSEE/2FPS/DataSet/Train/110004/11...</td>\n",
       "      <td>1100042011</td>\n",
       "      <td>1100042011.avi</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>110004201115.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../../data/DAiSEE/2FPS/DataSet/Train/110004/11...</td>\n",
       "      <td>1100042011</td>\n",
       "      <td>1100042011.avi</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>110004201114.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../../data/DAiSEE/2FPS/DataSet/Train/110004/11...</td>\n",
       "      <td>1100042011</td>\n",
       "      <td>1100042011.avi</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>110004201116.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../../data/DAiSEE/2FPS/DataSet/Train/110004/11...</td>\n",
       "      <td>1100042011</td>\n",
       "      <td>1100042011.avi</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>110004201117.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../../data/DAiSEE/2FPS/DataSet/Train/110004/11...</td>\n",
       "      <td>1100042011</td>\n",
       "      <td>1100042011.avi</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11000420119.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                file        root  \\\n",
       "0  ../../data/DAiSEE/2FPS/DataSet/Train/110004/11...  1100042011   \n",
       "1  ../../data/DAiSEE/2FPS/DataSet/Train/110004/11...  1100042011   \n",
       "2  ../../data/DAiSEE/2FPS/DataSet/Train/110004/11...  1100042011   \n",
       "3  ../../data/DAiSEE/2FPS/DataSet/Train/110004/11...  1100042011   \n",
       "4  ../../data/DAiSEE/2FPS/DataSet/Train/110004/11...  1100042011   \n",
       "\n",
       "           ClipID  Boredom  Engagement  Confusion  Frustration  \\\n",
       "0  1100042011.avi      2.0         2.0        1.0          1.0   \n",
       "1  1100042011.avi      2.0         2.0        1.0          1.0   \n",
       "2  1100042011.avi      2.0         2.0        1.0          1.0   \n",
       "3  1100042011.avi      2.0         2.0        1.0          1.0   \n",
       "4  1100042011.avi      2.0         2.0        1.0          1.0   \n",
       "\n",
       "           basefile  \n",
       "0  110004201115.jpg  \n",
       "1  110004201114.jpg  \n",
       "2  110004201116.jpg  \n",
       "3  110004201117.jpg  \n",
       "4   11000420119.jpg  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write dataframe to pickle in case we need it later\n",
    "df_train.to_pickle(out_dir + \"/df_train.pkl\")\n",
    "df_test.to_pickle(out_dir + \"/df_test.pkl\")\n",
    "df_val.to_pickle(out_dir + \"/df_val.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = df_train['file_str'] = df_train['file'].to_string()\n",
    "filepath = df_test['file_str'] = df_test['file'].to_string()\n",
    "filepath = df_val['file_str'] = df_val['file'].to_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_arrays(df, usage):\n",
    "    np.random.seed(100)\n",
    "    indices = np.random.permutation(len(df['file']))\n",
    " \n",
    "    filepath = df['file_str'].to_numpy()\n",
    "    filepath = filepath[indices]\n",
    "    label = np.array(df[['Boredom', 'Engagement', 'Confusion', 'Frustration']]) \n",
    "    label = label[indices]\n",
    "\n",
    "    np.save(f\"{str(out_dir)}/x_{usage.lower()}\", filepath, allow_pickle=True)\n",
    "    np.save(f\"{str(out_dir)}/y_{usage.lower()}\", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_arrays(df_train, 'train')\n",
    "save_arrays(df_test, 'test')\n",
    "save_arrays(df_val, 'validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move files to appropriate directories (train, test, val)\n",
    "\n",
    "### Create Class Subdirectories for ease of use with Tenrorflow datasets\n",
    "\n",
    "Inititally treat as a binary problem (cannot be a bit engaged and a bit bored  \n",
    "Therefore create a binary class column based on boredom  \n",
    "If there are duplicated we are going to keep going, because one less image file should not make a difference. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Boredom\n",
       "0.0    24330\n",
       "1.0    16960\n",
       "2.0    10730\n",
       "3.0     1560\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check values for 'Boredom'\n",
    "df_train.groupby(['Boredom']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encode\n",
    "y_train = pd.get_dummies(df_train['Boredom'], prefix='b')\n",
    "y_test = pd.get_dummies(df_test['Boredom'], prefix='b')\n",
    "y_val = pd.get_dummies(df_val['Boredom'], prefix='b')\n",
    "\n",
    "df_train = pd.concat([df_train,y_train], axis = 1)\n",
    "df_test = pd.concat([df_test,y_test], axis = 1)\n",
    "df_val = pd.concat([df_val,y_val], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b_0.0\n",
       "0    30490\n",
       "1    24330\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check counts of labels\n",
    "df_train.groupby(['b_0.0']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.rename(columns={\"b_0.0\": \"b0\", \"b_1.0\": \"b1\", \"b_2.0\": \"b2\", \"b_3.0\": \"b3\"}, inplace = True)\n",
    "df_test.rename(columns={\"b_0.0\": \"b0\", \"b_1.0\": \"b1\", \"b_2.0\": \"b2\", \"b_3.0\": \"b3\"}, inplace = True)\n",
    "df_val.rename(columns={\"b_0.0\": \"b0\", \"b_1.0\": \"b1\", \"b_2.0\": \"b2\", \"b_3.0\": \"b3\"}, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple function to copy images to correct file structure\n",
    "# As we have some duplicate file names, instead of fixing we will ignore as we have more than enough images\n",
    "def copy_files(source, destination):\n",
    "    for f in source:\n",
    "        try:\n",
    "            destination_file = os.path.join(destination, os.path.basename(f))\n",
    "            shutil.copy(os.fspath(f), destination_file)\n",
    "        except: \n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create image file structure for bored/not bored\n",
    "cols = ['b0', 'b1', 'b2', 'b3']\n",
    "dirs = ['train', 'test', 'validation']\n",
    "\n",
    "for d in dirs:\n",
    "    for c in cols:\n",
    "        data_dir = out_dir + '/' + d + '/' + c\n",
    "        if not os.path.exists(data_dir):\n",
    "            os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move Train Images\n",
    "copy_files(df_train[df_train['b0']==1]['file'].to_list(), out_dir + '/train/b0/')\n",
    "copy_files(df_train[df_train['b1']==1]['file'].to_list(), out_dir + '/train/b1/')\n",
    "copy_files(df_train[df_train['b2']==1]['file'].to_list(), out_dir + '/train/b2/')\n",
    "copy_files(df_train[df_train['b3']==1]['file'].to_list(), out_dir + '/train/b3/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move Test Images\n",
    "copy_files(df_test[df_test['b0']==1]['file'].to_list(), out_dir + '/test/b0/')\n",
    "copy_files(df_test[df_test['b1']==1]['file'].to_list(), out_dir + '/test/b1/')\n",
    "copy_files(df_test[df_test['b2']==1]['file'].to_list(), out_dir + '/test/b2/')\n",
    "copy_files(df_test[df_test['b3']==1]['file'].to_list(), out_dir + '/test/b3/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move Validation Images\n",
    "copy_files(df_val[df_val['b0']==1]['file'].to_list(), out_dir + '/validation/b0/')\n",
    "copy_files(df_val[df_val['b1']==1]['file'].to_list(), out_dir + '/validation/b1/')\n",
    "copy_files(df_val[df_val['b2']==1]['file'].to_list(), out_dir + '/validation/b2/')\n",
    "copy_files(df_val[df_val['b3']==1]['file'].to_list(), out_dir + '/validation/b3/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
